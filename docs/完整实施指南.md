# 完整实施指南

## 目录
1. [概述](#概述)
2. [环境准备](#环境准备)
3. [数据准备阶段](#数据准备阶段)
4. [目标检测阶段](#目标检测阶段)
5. [多目标跟踪阶段](#多目标跟踪阶段)
6. [嵌入式部署阶段](#嵌入式部署阶段)
7. [系统测试阶段](#系统测试阶段)
8. [消融实验](#消融实验)

---

## 概述

本指南详细描述了从数据集下载到最终部署的完整实施流程，确保项目能够系统、有序地推进。

### 系统架构

```
┌─────────────────────────────────────────────────────────────────┐
│                        数据输入层                                │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │   FLIR数据集    │  │   KAIST数据集   │  │  RV1126视频流   │  │
│  │   (离线训练)    │  │   (跟踪评估)    │  │   (实时部署)    │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│                        算法处理层                                │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                    目标检测模块                          │    │
│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐           │    │
│  │  │  Backbone │→ │   Neck    │→ │   Head    │           │    │
│  │  │(轻量化)   │  │ (PANet)   │  │(多尺度)   │           │    │
│  │  └───────────┘  └───────────┘  └───────────┘           │    │
│  └─────────────────────────────────────────────────────────┘    │
│                               ↓                                  │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                   多目标跟踪模块                         │    │
│  │  ┌──────────┐  ┌──────────┐  ┌───────────┐             │    │
│  │  │DeepSORT  │  │ByteTrack │  │CenterTrack│             │    │
│  │  └──────────┘  └──────────┘  └───────────┘             │    │
│  └─────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│                       嵌入式部署层                               │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │  PyTorch→ONNX   │→ │  ONNX→RKNN     │→ │  NPU推理程序    │  │
│  │   模型转换      │  │  INT8量化       │  │  (C++ API)      │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│                       性能评估层                                 │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │    检测指标     │  │    跟踪指标     │  │   系统指标      │  │
│  │ mAP/P/R         │  │ MOTA/IDF1/IDSW  │  │ FPS/延迟/CPU    │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 环境准备

### 1. PC端开发环境

#### 硬件要求
- CPU: Intel i5及以上
- 内存: 16GB及以上
- GPU: NVIDIA GTX 1080及以上 (推荐RTX 3090)
- 存储: 100GB可用空间

#### 软件要求
```bash
# 操作系统
Ubuntu 20.04 LTS (推荐) 或 Windows 10/11

# Python环境
Python 3.8.x

# CUDA环境
CUDA 11.3
cuDNN 8.2

# 深度学习框架
PyTorch 1.10.0+cu113
```

#### 安装步骤
```bash
# 1. 创建Python虚拟环境
python -m venv bishe_env
source bishe_env/bin/activate

# 2. 安装PyTorch
pip install torch==1.10.0+cu113 torchvision==0.11.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html

# 3. 安装项目依赖
pip install -r requirements.txt

# 4. 验证环境
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

### 2. RV1126开发环境

#### 硬件要求
- RV1126开发板
- 红外摄像头模组
- SD卡 (32GB及以上)
- USB调试线

#### 软件要求
```bash
# 交叉编译工具链
aarch64-linux-gnu-gcc 9.3

# RKNN Toolkit
rknn-toolkit2 1.4.0

# NPU驱动
librknnrt.so (开发板内置)
```

---

## 数据准备阶段

### 第一步：下载数据集

#### FLIR数据集（主要训练用）
```bash
# 运行下载脚本
python scripts/data/download_dataset.py

# 或手动下载
# 访问: https://www.flir.com/oem/adas/adas-dataset-form/
# 下载 FLIR Thermal Dataset 并解压到 data/raw/flir/
```

#### KAIST数据集（跟踪评估用）
```bash
# 访问: https://soonminhwang.github.io/rgbt-ped-detection/
# 下载多光谱行人检测数据集
# 解压到 data/raw/kaist/
```

### 第二步：数据预处理

```bash
# 准备FLIR数据集（转换为YOLO格式）
python scripts/data/prepare_flir.py \
    --input data/raw/flir \
    --output data/processed/flir \
    --split-ratio 0.8

# 准备KAIST数据集
python scripts/data/prepare_kaist.py \
    --input data/raw/kaist \
    --output data/processed/kaist
```

### 第三步：数据增强策略

针对红外图像特点设计专门的数据增强策略：

```yaml
# configs/dataset.yaml 中的数据增强配置
augmentation:
  # 基础增强
  hsv_h: 0.0  # 红外图像无颜色，关闭色调变换
  hsv_s: 0.0  # 关闭饱和度变换
  hsv_v: 0.3  # 亮度变换，模拟温度变化
  
  # 几何变换
  degrees: 10.0      # 旋转角度
  translate: 0.1     # 平移比例
  scale: 0.5         # 缩放范围
  shear: 5.0         # 剪切角度
  perspective: 0.0   # 透视变换
  flipud: 0.0        # 上下翻转
  fliplr: 0.5        # 左右翻转
  
  # 红外专用增强
  gaussian_noise: 0.02  # 高斯噪声（模拟传感器噪声）
  salt_pepper: 0.01     # 椒盐噪声
  contrast_adjust: 0.2  # 对比度调整（模拟温度变化）
  mosaic: 1.0           # Mosaic增强
  mixup: 0.1            # Mixup增强
```

---

## 目标检测阶段

### 第一步：基准模型训练

```bash
# 训练YOLOv5s基准模型
python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --data configs/dataset.yaml \
    --weights yolov5s.pt \
    --epochs 300 \
    --batch-size 32 \
    --img-size 640 \
    --device 0 \
    --name baseline
```

### 第二步：轻量化改进实验

按照控制变量法，依次进行以下实验：

#### 实验1：骨干网络替换
```bash
# 对比不同轻量化骨干网络
# 原生C3 vs Ghost-C3 vs Shuffle-C3

# 测试Ghost-C3
python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --backbone ghost \
    --name exp_backbone_ghost

# 测试Shuffle-C3
python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --backbone shuffle \
    --name exp_backbone_shuffle
```

#### 实验2：损失函数优化
```bash
# 对比不同损失函数
# CIoU vs SIoU vs EIoU

python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --loss siou \
    --name exp_loss_siou

python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --loss eiou \
    --name exp_loss_eiou
```

#### 实验3：注意力机制
```bash
# 对比不同注意力机制
# CBAM vs CoordAttention vs SE

python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --attention cbam \
    --name exp_attention_cbam

python scripts/train/train_yolov5.py \
    --config configs/train_config.yaml \
    --attention coordatt \
    --name exp_attention_coordatt
```

### 第三步：最优模型确定

```bash
# 评估所有实验结果
python scripts/evaluate/eval_detection.py \
    --weights-dir outputs/weights/ \
    --data configs/dataset.yaml \
    --output outputs/results/detection_comparison.csv
```

---

## 多目标跟踪阶段

### 第一步：集成跟踪算法

使用最优检测模型作为基础，测试不同跟踪算法：

```bash
# DeepSORT跟踪器
python scripts/evaluate/eval_tracking.py \
    --detector outputs/weights/best.pt \
    --tracker deepsort \
    --video data/processed/kaist/test_sequences/ \
    --output outputs/results/tracking_deepsort/

# ByteTrack跟踪器
python scripts/evaluate/eval_tracking.py \
    --detector outputs/weights/best.pt \
    --tracker bytetrack \
    --video data/processed/kaist/test_sequences/ \
    --output outputs/results/tracking_bytetrack/

# CenterTrack跟踪器
python scripts/evaluate/eval_tracking.py \
    --detector outputs/weights/best.pt \
    --tracker centertrack \
    --video data/processed/kaist/test_sequences/ \
    --output outputs/results/tracking_centertrack/
```

### 第二步：跟踪算法对比分析

```bash
# 综合对比三种跟踪算法
python scripts/evaluate/compare_trackers.py \
    --results-dir outputs/results/ \
    --output outputs/results/tracking_comparison.csv
```

### 跟踪场景测试

设计以下测试场景验证跟踪性能：

| 场景 | 描述 | 评估重点 |
|-----|------|---------|
| 正常跟踪 | 目标匀速移动，无遮挡 | 基础跟踪能力 |
| 目标交叉 | 多个目标轨迹交叉 | 身份保持能力 |
| 部分遮挡 | 目标被部分遮挡 | 遮挡处理能力 |
| 完全遮挡 | 目标短暂完全消失 | 重识别能力 |
| 非线性运动 | 目标快速转向、加速 | 运动预测能力 |

---

## 嵌入式部署阶段

### 第一步：模型导出

```bash
# 导出ONNX格式
python scripts/deploy/export_model.py \
    --weights outputs/weights/best.pt \
    --img-size 640 \
    --batch-size 1 \
    --simplify \
    --output outputs/weights/best.onnx
```

### 第二步：RKNN转换与量化

```bash
# 转换为RKNN格式（INT8量化）
python scripts/deploy/convert_to_rknn.py \
    --onnx outputs/weights/best.onnx \
    --output outputs/weights/best.rknn \
    --quantize int8 \
    --dataset data/processed/flir/calibration/ \
    --platform rv1126
```

### 第三步：PC端模拟测试

```bash
# 在PC端测试RKNN模型
python scripts/deploy/test_rknn.py \
    --model outputs/weights/best.rknn \
    --image data/processed/flir/test/images/ \
    --simulator
```

### 第四步：开发板部署

```bash
# 1. 编译嵌入式程序
cd embedded
mkdir build && cd build
cmake .. -DCMAKE_TOOLCHAIN_FILE=../toolchain.cmake
make -j4

# 2. 传输到开发板
scp -r build/* root@rv1126:/opt/infrared_tracker/
scp ../configs/* root@rv1126:/opt/infrared_tracker/configs/
scp ../../outputs/weights/best.rknn root@rv1126:/opt/infrared_tracker/models/

# 3. 在开发板运行
ssh root@rv1126
cd /opt/infrared_tracker
./infrared_tracker --config configs/deploy.yaml
```

---

## 系统测试阶段

### 第一步：离线测试

```bash
# 检测性能评估
python scripts/evaluate/eval_detection.py \
    --weights outputs/weights/best.pt \
    --data configs/dataset.yaml \
    --task val \
    --verbose

# 跟踪性能评估
python scripts/evaluate/eval_tracking.py \
    --detector outputs/weights/best.pt \
    --tracker [选定的最优跟踪器] \
    --video data/processed/kaist/test_sequences/ \
    --metrics mota,idf1,idsw
```

### 第二步：实时测试（在RV1126上）

```bash
# 连接到开发板进行测试
ssh root@rv1126

# 运行性能测试
cd /opt/infrared_tracker
./performance_test \
    --model models/best.rknn \
    --duration 60 \
    --output /tmp/performance_results.json
```

### 第三步：性能指标收集

```bash
# 收集并汇总所有性能指标
python scripts/evaluate/collect_metrics.py \
    --detection-results outputs/results/detection_eval.json \
    --tracking-results outputs/results/tracking_eval.json \
    --system-results outputs/results/system_performance.json \
    --output outputs/results/final_report.html
```

---

## 消融实验

### 实验设计

采用逐步累加方式验证技术路线：

```bash
# 运行消融实验脚本
python scripts/train/ablation_study.py \
    --config configs/train_config.yaml \
    --output outputs/results/ablation/
```

### 实验步骤

| 步骤 | 配置 | 说明 |
|-----|------|------|
| 基准 | YOLOv5s + DeepSORT | 原生算法组合 |
| +轻量化骨干 | Ghost-C3替换 | 减少计算量 |
| +损失函数 | SIoU/EIoU | 提升定位精度 |
| +注意力机制 | CBAM/CoordAtt | 增强特征提取 |
| +最优跟踪器 | ByteTrack/其他 | 提升跟踪稳定性 |
| +INT8量化 | RKNN量化 | 验证量化影响 |

### 结果分析

```bash
# 生成消融实验报告
python scripts/evaluate/analyze_ablation.py \
    --results outputs/results/ablation/ \
    --output outputs/results/ablation_report.html
```

---

## 注意事项

1. **版本控制**：每次重要实验后，保存模型权重和配置文件
2. **日志记录**：所有训练和测试过程自动记录到`outputs/logs/`
3. **可复现性**：固定随机种子，确保实验可复现
4. **资源管理**：注意GPU显存和磁盘空间的使用

## 常见问题

### 问：训练时出现CUDA内存不足？
答：减小batch-size或图像尺寸，或使用梯度累积

### 问：RKNN转换失败？
答：检查ONNX模型中是否有不支持的算子，查看RKNN兼容性文档

### 问：开发板推理速度达不到要求？
答：优化前后处理代码，检查内存拷贝开销，考虑进一步模型量化
